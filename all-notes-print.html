<html lang="en">
 <head>
    <meta charset="utf-8">
  <head>
  <body>
    <h1>
      <code style="font-size: smaller">
	<a href="https://pypi.org/project/uncompyle6/">uncompyle6</a>
      </code>
      and <code style="font-size: smaller">
	<a href="https://github.com/rocky/python-decompile3/">decompyle3</a></code>:
      How to Read and Write a High-Level Bytecode Decompiler
    </h1>
    <p>I have a lot of material to get through it all. Time is going
    to be short. Some of what I have to say is very compact and
    abbreviated. I apologize in advance.
    </p>
    <p>If you can, I <em>strongly</em> recommend you follow along with
      the printed version of the text. So let’s begin...
    </p>
    <hr/>

    <!--- 010-intro.html #/1 -->
    <h1>Survey</h1>

    <p>Please raise your hand if use have used Hex-Rays Ida Pro,
    Ghidra or Binary Ninja? Awesome!</p>

    <p>How about using any of these to decompile Python
    bytecode to Python source text?</p>

    <p>As far as I know, there is no support for Python decompilation
      in IDA Pro, Ghidra or Binary Ninja. In preparation for this
      talk, I spoke to one of the founders from one of these
      reverse-engineering tool makers who confirmed this. More
      generally, he says that all these kinds of
      decompiler tools which start from machine code are
      pretty bad when it comes to any interpreter
      that runs off bytecode.</p>

    <p>Now please raise your hand if you have
    used <em>uncompyle6</em>, or <em>decompyle3</em>. [*]</p>

    <hr/>

    <!--- 010-intro.html #/1/1 -->

    <h1>Github commit statistics for <code style="font-size: smaller">uncompyle6</code></h1>

    <p>I’m the current maintainer and developer
      of <em>uncompyle6</em>, and <em>decompyle3</em>.  I’m sorry for
      my really bad choice of names for the projects.
    </p>

    <p>This page shows github commit activity for <em>uncompyle6</em>
      from 2012 to now. 31 people have contributed to this open-source
      project, usually submitting one or two pull requests. These
      are greatly appreciated. Keep up the good work! But
      over the vast sea of time, I have been working on this largely
      by myself. My contributions are in the bottom left. The person who
      is ranked second in commits, moagstar, is to the right. I
      haven’t heard from him in a little over 7 years.</p>

     <p>Back in 2016, I started this project on github from
      a "tar" of a copy of an archive from an
      obsolete version-control system. The code had been abandoned for
      about 5 years. And that accounts for the big gap at the left.
    </p>

    <p>In 2018, I gave a talk at Python conference. So you see another
      flurry of activity for 2018 in the middle of the graph.
    </p>

    <p>
      This code works for Python bytecode from the very first version
      in 1999 to Python version 3.8 which is no longer
      current. It came on the scene a few years ago but it
      is still supported and is still in use.
    </p>

    <p>
      Since code is old and large, and Python compilers have gotten more
      sophisticated, the code needs better technology to keep up with
      Python. So I forked the code, refactored it, and reduced its
      size. That accounts for the drop off in activity you see on the
      right part of the graph from 2021 to now. The forked code goes
      under the name decompyle3. It only handles Python 3.7
      and 3.8.
    </p>

    <p>
      A little fun fact: on the extreme right, you see a little
      uptick. That is me working on the code in preparation for this
      talk.
    </p>

    <p>
      Unfortunately, even changes to decompyle3 weren’t aggressive
      enough to track changes to Python bytecode and the Python
      language. Therefore, I have an experimental decompiler
      for Python 3.8 to 3.10.  Towards the end, I will show a little
      bit of that. [*]
    </p>
    <hr/>

    <!--- 010-intro.html #/1/2 -->
    <h1>So, who am I?</h1>
    <p>
      Who am I?</p>
    <p>
      That is me in the photo in this cartoon down at the bottom of
      the tower. Except I live in New York City, and I have been
      doing open-source work since 2000.</p>

    <p>
      When not doing my day job, I have been writing open-source
      software. But now that I’m retired, I spend even more time on
      this.</p>
    <hr/>

    <!--- 010-intro.html #/1/3 -->
    <h1>Some Open Source Software that Includes My Code:</h1>

    <p>So the code I've written in general has had plenty of
      time to spread around. If you have have software installed that
      matches any of the logos on the first line, then you have code
      I've written.</p>

    <p>I don't have any cool logos for the software that I’m going to
      be talking about today. But the software used by the projects
      with the logos on the left, all use a CD-reading library,
      called with the logos on the left, use a CD-reading library,
      called "libcdio" that I started in 2000 and still maintain.
    </p>
    <p>
      And I’m interested in other things like an open-source
      Mathematica, and debuggers. Most of the software I have written
      is GPL and I am a member of the Free Software Foundation.
    </p>
    <hr/>

    <!--- 020-background.html #/2 -->
    <h1>Background</h1>
    <p>
      There is a big need for and interest in using decompilation to
      understand malware. A couple of people from Microsoft a while
      back, contacted me regarding a malware botnet that was
      prevalent on Microsoft Windows. The malware was written in
      Python 2.7. As I mentioned before, the standard analysis tools
      like Hex Rays or Ghidra were of no use to them.
    </p>

    <p>
      And the situation has been getting worse. The language and
      bytecode continue to change. Python’s generated bytecode has
      become more complex and harder to decompile.  In general, this
      is the case with other programming languages as
      well.
    </p>

    <p>
      Decompilers have been around since the dawn of programming
      languages in the early 1960's. [*]</p>

    <p>
      However, the ideas I’m going to present here are somewhat new or
      little used, and represent my own personal research. The seeds
      of the ideas were buried in code going back 24 years.  As I
      said, that code was abandoned for 5 years before I came across
      it. It took me a while to distill, correct, and expand upon the
      ideas embodied in the code. I’m not even sure I now have the
      final word on this. [*]
    </p>


    <p>
      There is theory and academic research for what I
      call general-purpose decompilers. These are programs
      that convert machine code into a pseudo-source
      code. Usually, the resulting language is not something
      that you can directly run through a compiler
    </p>

    <p>
      However, for languages that work off of high-level bytecode,
      like Python, Ruby, or Ethereum VM, there is very
      little general theory other than
      what I have written and talked about. The last talk I gave was 5
      years ago, and much has happened since then. [*]
    </p>
    </hr>

    <!--- 020-background.html #/2/1 -->
    <h1>Books on making compilers:</h1>

    <p>
      There are systematic methods and tools for compiler
      construction. These are just some of the books you can
      find on amazon.com. Some of these have gone through several
      editions over decades. Other books come in several variations
      based on which programming language the book’s compiler
      algorithms are written in. One expresses its algorithms in C,
      another in ML, and another in Java. Some books specialize in
      writing a compiler targeting a specific programming
      language, like C. Or compilers running on a
      specific CPU architecture, or a
      particular operating system. Some books specialize in
      writing interpreters.  Some books are aimed at advanced-compiler
      technology. And finally, some are no longer in print, because
      compiler-writing technology has evolved over time.
    </p>

    <p>I think you get the idea that this is a well-trodden field. [*]
    </p>

    </hr>
    <!--- 020-background.html #/2/2 -->
    <h1>Books on making decompilers:</h1>
    <p>None of the compiler books in the last slide mention
      "decompiler", as far as I know.</p>

    <p>So how about books on decompilers? [*]
    </p>

    <p>
      For decompiler construction, these are the only two I could
      find. Both are about a decade old. Neither is 100%
      devoted to decompilers, and neither has gone through more than
      one edition.</p>

    <p>
      Nevertheless, the topic of how to write a decompiler is about as
      rich, deep, and involved as the subject of how to write a
      compiler. In fact, a decompiler is a kind of compiler. Both take
      code expressed in one language and translate it to equivalent
      code in another language.  Conventional compiler wisdom,
      however, needs to be adapted.  You’ll get a small sense of this
      in this talk. [*]
    </p>

    </hr>

    <!--- 020-background.html #/2/3 -->
    <h1>Can AI Save the Day?</h1>
    <p>
      Because of the need for, but scarcity of, decompilers, AI might
      seem like a possible way to get high-level information
      from machine code.
    </p>

    <p>For this talk, I recently looked at two Python decompilers that
      use machine learning. Can AI save the day?
    </p>

    <p>
      Well... No, not yet. If you are interested in how to fix this, I can
      share my ideas afterwards. [*]
    </p>

    </hr>

    <!--- 030-what-talk-is-about.html #/3 -->
    <p>Why am I here? Why am I giving this talk?</p>

    <p>
      I’ve mentioned that general-purpose decompilers found in Ghidra,
      Binary Ninja, or Hex Rays are of little-to-no use for bytecode
      languages. In fact, I have been trying to raise awareness of
      bytecode decompilation as its own thing. And that’s a big reason
      I am here. As I have come to understand, bytecode decompilation
      works very differently from general-purpose decompilation.
    </p>

    <p>
      Adding the modifier "General Purpose" is my own
      terminology. Right now, this distinction is not widely
      accepted.
    </p>

    <p>A couple years ago, to raise awareness, I started a discussion
      in the Wikipedia "talk" section for
      "Decompilers". Unfortunately, there hasn’t been much discussion
      or movement on this. [*]
    </p>

    <p>I will soon describe the decompilation process, and I will
      introduce a rarely used, if not entirely new, approach
      to decompilation. I treat decompilation as a human-language
      translation problem. In some ways, it can be like the kind of
      thing Google Translate does. [*]
    </p>

    <p>
      But the process we will go through, also follows a
      pattern similar to the process used by compiler front-ends,
      including the compiler for Python.  It is described in all of
      the compiler books that I showed images of before. I am going to
      show this process adapted to perform decompilation. And
      I think this kind of adaption can be extended to create
      decompilers for other high-level bytecode. [*]
    </p>
    <hr/>

    <!--- 030-what-talk-is-about.html #/3/1 -->
    <h1>Key Takeaways</h1>

    <p>At a more technical level, I hope you understand the phases of
      decompilation in these Python decompilers. This can be useful in
      submitting bug reports or fixing bugs.
    </p>

    <p> A common misconception I see on reverse-engineering forums is
      confusion between decompilation and disassembly. As we go through
      the process, I think the differences will be
      clear.
    </p>

    <p>
      For those of you are familiar with machine code, I
      think you will appreciate how high-level bytecode different from
      machine code. [*]
    </p>

    <p>
      Finally, I think you will get some idea of the things
      decompilers can do and cannot do, not only in Python
      but other similar programming languages.
    </p>

    <hr/>

    <!--- 040-python-example.html #/4 -->
    <h1>Simple Python Program</h1>
    <p>
      Before I get too deep into Python decompilation, I
      should say something about what it
      is. Briefly, decompilation starts with bytecode as
      input and produces source text as output. In this slide, we have
      some Python. So where does the bytecode come in?  That gets
      introduced in a second.
    </p>

    <p>Let me first start off with some basics that go on
      when you run a Python program.
    </p>

     <p>
      I’m going to use this simple program in the examples later. The
      programs prints the output of calling a function
      named <em>five()</em> [*]
    </p>

    <p>
      I run the code using the usual Python interpreter called
      (CPython) and we get the expected output: 5.
    </p>

    <p>
      When I do this, under the covers Python translates or
      <em>compiles</em> the program into an intermediate
      form called <em>bytecode</em> and interprets that
      internal form. In this particular case, that bytecode stays
      internal; it isn’t written out to disk.</p>

    <p>
      Here are the bytecode bytes for the main routine. What these
      instructions do is call for the creation of the
      function <em>five()</em>. In an interpreted language, there is
      no linker loader, like you have in statically-compiled languages
      &mdash;the tools that general-purpose analysis tools handle.
      Instead, this kind of thing is done as the code runs. After
      linking function <em>five()</em> into the main program, the
      function is called and the function return value is passed to
      the built-in <em>print()</em> function which displays the output
      shown.
    </p>

    <p>
      The instructions for doing all of that stuff that I just said,
      take up only 25 bytes!</p>

    <p>For those of you who are familiar with machine code, 25 bytes
      to do all of this is, like, awesome! There are metadata
      sections that are in the bytecode and that adds a bit of space
      too. I haven't shown that here, but we will see some of this a
      little bit later.</p>

    <p>
      Everything, overall, is very very compact. Part of the
      compactness is that the individual instructions themselves are
      pretty high level. And we’ll see some of that later as
      well. [*]
    </p>

    <hr/>
    <!--- 050-compile-and-run-bytecode.html #/5 -->
    <p>
      As I said, the interpreter doesn’t run the source text directly,
      but instead runs bytecode. One implication of this is that the
      source text doesn’t need to exist after byte-code compilation is
      done.
    </p>
    <p>For certain kinds of Python source text, called "modules", the
      internal bytecode is written to disk automatically.</p>
    <p>
      However, we can force the bytecode to get written to
      disk using the Python
      <code>compileall</code> module. This is the invocation.
    </p>

    <p>
      A "bytecode file" is a built-in Python code object which is
      serialized and then written out a file using a routine called
      "marshal". There is a little extra data added like the
      source-text file name, and bytecode version. [*]
    </p>

    <p>The short name of this file here is 5.cpython-38.pyc. The extension of a
      bytecode file usually ends in .pyc as it is here, or .pyo . [*]
    </p>

    <hr/>

    <!--- 050-compile-and-run-bytecode.html #/5/1 -->
    <h1>Running Python Bytecode</h1>
    <p>Now that we have this bytecode object, I can run just the
      bytecode.
    </p>

    <p>
      And again, I get 5.  To show that the source code is no longer
      needed, I will move the bytecode to a different location and
      remove the Python source text file.
    </p>

    <p>
      I run the bytecode, and again I get the same output 5.
    </p>

    </hr>
    <!--- 060-bytecode-decompilation-example.html #/6 -->
    <h1>Python Bytecode Decompilation Example</h1>

    <p>
      With this introduction, we can now do our first Python
      decompilation using <em>uncompyle6</em>. The simplest invocation
      is: <em>uncompyle6</em> with a bytecode file name, here
      "/tmp/five-moved.pyc". This is what we created in the last slide.
    </p>

    <p>
      Lines in orange that start with <code>#</code> are comments. The
      section at the top contains a little bit of the meta-data that I
      mentioned before, which is stored in the bytecode file. It
      is not part of that 25-byte instruction sequence that we
      saw in hex before.
    </p>

    <p>
      Each bytecode has a unique number for the bytecode variant. Here,
      it is 3413, and this variant covers Python version 3.8.
    </p>

    <p>
      Major releases of Python typically can alter the
      programming language and/or the bytecode a little.
    </p>

    <p>
      Python bytecode varies more than any other bytecode that
      I have encountered. So when you find a tool or read a blog about
      Python bytecode, some of the details might only be relevant for a
      small number of versions around the time that the tool was
      developed or when the blog was written.
    </p>

    <p>
      The drift in bytecode from the first release in 1996 to the most
      recent version is about as dramatic as the drift from Latin to
      Italian.
    </p>

    <p>
      The name of the bytecode file is "five-moved", seen in
      white at the top.  The name of the original Python file is
      "five.py" is seen in orange, a little down from the top. [*]

    <p>
      But if you ignore the orange comments in both the source Python
      and the reconstructed Python shown here, you will see that the two
      are very very close.</p>

    <p>
      And let me show that side by side...
    </p>

    <hr/>
    <!--- 070-source-code-differences.html #/7 -->
    <h1>Source Code Differences</h1>

    <p>
      As you can see here, the source and the decompiled code are
      pretty much the same.

    <p>
      People who are used to general-purpose decompilers, such as the
      ones that you find in Ghidra, or Hex Rays, are
      usually amazed at how close the two are. This is one facet of
      high-level bytecode: a lot of source information like the
      variables names along with their types is preserved inside the
      bytecode.
    </p>

    <p>
      The main differences between the two are the comments
      produced. Comments from the source program don’t appear in the
      reconstructed result. That is because those comments don’t appear
      anywhere in the bytecode.
    </p>

    <p>For example, line 11 on the left doesn’t appear anywhere on the
      right.</p>

    <p>
      One thing these decompilers do to increase the
      likelihood of getting exact source text is that they
      make an attempt to format the way standard formatters for Python
      work. We are more careful about this than other Python
      decompilers. But we don't always get the formatting exactly the
      same. [*]
    </p>

    <hr/>

    <!--- 080-how-decompilation-works.html #/8 -->
    <h1>How These Decompilers Work</h1>

    <p>
      Now that we’ve given an example of Python decompilation, next we
      show how our decompilers work. With this, you can then follow
      along the decompiler’s thought process.</p>

    <p>
      <i>One of the unique features of these decompilers, is that we
      provide a means for following along the decompilation
      process.</i> Machine-language decompilers, even though they
      have a language-based affinity like we have, currently have no
      way to provide this level of detail.
    </p>

    <p>
      Our decompilers go through five phases. This idea of running
      through phases or constructing a pipeline is also how most
      compilers work.</p>

    <p>The phases are:
      <ol>
	<li>Get bytecode disassembly
	using <em>xdis</em>. <em>xdis</em> is the cross-version
	disassembly library that I wrote to be able to
	support this project in particular and also to support bytecode
	analysis in general. [*] </li>
	<li>"Tokenize" the disassembly. "Tokenize" is a
	compiler-centric term. In other decompilers and code-analysis
	tools, this process is sometimes called <em>lifting</em>,
	as in "lifting the disassembly" or "lifting the machine
	code". [*]</li>
	<li>Parse tokens to create a parse tree. [*]</li>
	<li>Abstract the parse tree to an "abstract syntax tree", and
	finally: [*]</li>
	<li>Produce Python source text from the abstract syntax tree.</li>
      </ol>
    </p>

    <p>Don’t worry if you don’t understand some of the above steps. I
      will go over each step in more detail soon.</p>

    <p>
      The scanning and parsing phases in the second and third step are
      similar to the beginning steps that compilers use to produce
      code. And this is the part that is most differentiates these
      decompilers from other Python decompilers. [*]
    </p>

    <hr/>

    <!--- 090-pydisasm-example.html #/9 -->
    <h1>Disassembly using
      <code
	class="hljs-title"
	style="font-size: smaller">
	pydisasm
      </code>from
      <a href="https://pypi.org/project/xdis/"
	 style="font-size: smaller">
	xdis
      </a>
    </h1>

    <p>
      The first step in decompilation is extracting a list of bytecode
      instructions, from bytecode bytes. Here, I will use the
      bytecode file that we generated earlier as input. And for
      demonstration purposes, I will use the standalone program
      "pydisasm" from the cross-version disassembler
      package. [*]
    </p>

    <p>
      First you’ll see that, again, there are comments in orange at the top.
      And again, this is metadata about the bytecode
    </p>
    <p>
      In fact, this particular metadata is exactly the
      same. For example, the version of bytecode that have here is, as
      before, 3413.
    </p>

    <p>
      Now let’s get to the actual Python bytecode
      instructions...
    </p>

    <p>
      I’ll go into this in more detail later. But the main thing to
      note here is that the first two instructions com from
      line one of the Python source text. Line one is indicated by the
      "1:" in white to the left.
    </p>

    <p>
      In the part that is greyed out below the one colon, you may see a "6:".
      That is the beginning of the source text for line 6.
    </p>

    <p>
      Each bytecode instruction consists of an operation name in
      blue. For example "LOAD_CONST", or "STORE_NAME". The instruction
      is then followed by an optional operand. These are listed in
      parentheses. [*]
    </p>

    <hr/>

    <!--- 100-parsing-tokens.html #/10 -->
    <h1>Phase 3: Parsing Tokens into a Parse Tree</h1>
    <p>
      In the left panel, you can see instructions that result from
      decoding bytecode, using a library routine from
      the <em>xdis</em> package. A small detail is that the decompiler
      doesn’t call a command-line routine but instead users a library
      API. So it has a disassembly structure.  On the left we
      are showing the print representation corresponding to the
      command-line utility.
    </p>

    <p>From this disassembly structure, we need to massage a little
      and repackage the information to put it into a form for input
      that parsers typically use.  This is shown in the right
      panel. Even though the two look almost the same, internally they
      are different structures. On the left is
      output of phase 1, and on the right is parser
      input in phase 2.
    </p>

    <p>
      Parsing will be coming up in the next slide.
    </p>

    <p>
    The input to the parser is a stream of "tokens". "Token" is the
    standard compiler term for the input atoms that are fed into a
    parser. A token is based on the operation name in blue. In fact,
    many times the instruction operation and token name are the
    same. However, operand, the field in green to the
    right, sometimes has parts of it folded into the
    token name. Here are examples of this. [*]
    </p>

    <p>
    In the first instruction, "LOAD_CONST" at offset 0 (offset numbers
    are in purple), the operation has been specialized to
    "LOAD_STR". In general, a "LOAD_CONST"' operand can
    be any Python constant literal, not just a string.</p>

    <p>This aspect shows a difference between bytecode and machine
    instructions. In machine code, operands are register values which
    can be numbers, addresses, or parts of addresses. In
    Python bytecode, operands are arbitrary
    Python objects!</p>

    <p>In the highlighted instructions, the "LOAD_CONST" operand
    is a string. We reflect this in the token name by
    changing the token name from "LOAD_CONST" into "LOAD_STR".
    </p>

    <p>The other operation name that is different from its
    corresponding token name is "MAKE_FUNCTION".  And you see that,
    down at the bottom at purple offset 8.  It gets a suffix added to
    the name and becomes "MAKE_FUNCTION_0". Zero is the
    bytecode-encoded way to indicate that the function signature for
    this function doesn’t take any parameters. The instruction
    sequence instructions leading up to "MAKE_FUNCTION" would
    change depending on the number of parameters a function
    takes. So a parser needs this rough parameter information to be
    able to match or parse the instructions leading up to the
    "MAKE_FUNCTION". [*]
    </p>

    <!--- 110-parsing-tokens.html #/11 -->
    <h1>Phase 3: Parsing Tokens Parse Tree</h1>

    <p>
      In the last slide, we showed the first two phases of
      decompilation. Those phases turned bytecode bytes into
      bytecode instructions, and then into a stream of
      tokens. The process next of scanning and parsing, that
      I will show now, will look familiar to those who have developed
      compilers. But that is kind of a niche market, so I suspect that
      for many of you, this will be new. If things go too
      quickly, you may find some of this explained in one of those
      compiler books of the images I showed earlier.
    </p>

    <p>
      To simplify things, we will focus on just the source text from
      the very first line. Even though this is a small part
      of the code, it covers all of the basic
      concepts very well.
    </p>

    <p>
      The companion materials has more detail if you want to see more
      complex examples. But in order to run, it helps to walk
      first. So let's start out with first line of the Python
      program. It was just a simple Python docstring shown in green at
      the top.</p>

<p>And let's start out with the first instruction. [*]
</p>

    <p>
      Note that we just use the token name "LOAD_STR". The string
      value isn’t relevant in parsing.
    </p>

    <p>
      That one instruction, all by itself, is the complete
      representation in bytecode for a particular Python expression.
      Here, it would be what Python will produce for a literal string
      expression, "Blackhat Asia Example", with nothing else.</p>

    <p>
      For those of you who are familiar with programming-language
      systems that compile to machine code, again this is
      strange.  In machine code, you rarely have one
      instruction fully covering one source-code expression. That's
      what makes this high-level bytecode.
    </p>

<p>Of course, many or most Python expressions require more than
  one instruction to represent them in
      bytecode.  [*]
    </p>

    <p>
      Because "LOAD_STR" is a complete expression, the parser matches
      that single token and emits a "grammar reduction
      rule". This is the "expr colon colon LOAD_STR" you see
      here. This line is written in Backus-Naur form. You will see
      this notation used in both Python's grammar specification and
      its abstract syntax tree specification.  More generally, this is
      the way syntax is specified in most programming languages.</p>

    <p>But the decompilers I've written also use grammars to
      do their work. And this is one of the 50 or so grammar rules for
      decompiling Python 3.8.</p>

      <p>
	The word to the right of "::=" in yellow is called
	a <em>grammar symbol</em> . The ::= is
	pronounced "transforms into" because an "expr" grammar symbol
	can "transform into" a LOAD_STR token. But since we are
	working bottom up (or token up), the parser recognizes this
	reduction possibility only after the fact. So the
	recognition is from the right to left, or alternatively, from
	the token to grammar symbol... [*]
     </p>

    <p>
      In performing this reduction rule, the parser creates a simple
      tree created with the "expr" grammar symbol in yellow as a
      root node of this tree, and "LOAD_STR" token in blue is its
      single child.  Now let's continue...
    </p>


    </hr>
    <!--- 110-parsing-tokens.html #/11/1 -->
    <h1>Parsing Tokens to Parse Tree (Part 2)</h1>
    <p>
      Here, I've just copied what we had before. Now let us get to the
      next bytecode instruction.
    </p>

    <p>
      A "STORE_NAME" instruction is one of the tokens that fully forms
      a "store" grammar symbol
    </p>

    <p>
      Although you won't find a "store" grammar symbol in a grammar
      for Python, you will find "Store" with a capital "S" appearing
      in Python's AST documentation. "Expr" with a capital E also
      appears in Python's AST.
    </p>

    <p>
      When possible, we try to use the grammar-symbol names that are
      similar to the names that Python's AST uses. The two are
      similar, but they have to be different.
    </p>

    <p>
      If instead of "STORE_NAME" we had "STORE_GLOBAL" , that
      would be another kind of instruction in the class of "store"
      grammar symbols.  In English, think of how a noun could be one
      of many words, like "bike", "car", or "bus". Let’s say that
      "STORE_NAME" is like "bike" while "STORE_GLOBAL" is like
      "car". As far as a sentence goes, bikes and cars are
      both nouns; "noun" is the grammar symbol that they both
      belong to. The grammatical structure of a sentence does not
      change when you replace "bus" for "car" in that sentence.
    </p>

    <p>
      To recap, we have now encountered two reduction rules. These are
      lines with yellow to the left. After this second rule is
      encountered, the little graph that the parser has been building
      looks like this: ...
    </p>

    <p>
      We now have two little trees. So let's continue...
    </p>

    <hr/>
    <!--- 110-parsing-tokens.html #/11/2 -->
    <h1>Parsing Tokens to Parse Tree (Part 3)</h1>
    <p>
      Again, the parser has matched "expr" and "store" grammar
      symbols, and built these trees for them.</p>

    <p>After finishing with the "store" reduction rule, the parser now
      notices that we have matched "expr" and "store" grammar symbols
      in succession. The grammar I wrote for the decompilers dictate
      that when these two are seen in succession, that constitutes an
      assignment statement. So another rule is triggered. [*]
    </p>

    <p>
      And the two trees that we had before are joined. [*}
    </p>

    <!--- 110-parsing-tokens.html #/11/3 -->
    <h1>Parsing Tokens to Parse Tree (Part 4)</h1>
    <p>
     To recap: We basically matched an assignment
      statement. [*]
    </p>

    <p>
      Continuing, an assignment statement is a kind of statement. And
      we have a grammar rule to indicate that.</p>

    <p>Notice that after seeing the second token "STORE_NAME" in blue
      from early on, we kicked off a flurry of reduction rules. This
      kind of thing happens in bottom-up parsers when you get to the
      end of the token stream. It is like a detective story where, in
      the last couple of pages, all of the loose fragments and
      mysteries of the story from the very beginning are finally
      wrapped up.
    </p>

    <p>
      Here is the final tree... And now let's see how this is shown as
      ASCII debug output... *
    </p>
    <hr/>

    <!--- 110-parsing-tokens.html #/11/4 -->
    <h1>Parse Tree to Source Code</h1>

    <p>This is an ASCII representation of the graph from the last
      slide.  It has more information shown than we saw in the graph.
      That is because we are also showing
      token attributes in addition to the
      token names.
    </p>

    <p>
      For example, we now see instruction offsets and instruction
      operand values. Grammar parsing uses just token names,
      but tokens themselves do store these other bits of information.
    </p>

    <p>
      All of this is good, but recall that we didn’t actually have an
      assignment statement at the top of the source code. Instead,
      what we had was a docstring.  However, docstrings are
      implemented in Python by creating a special
      kind of assignment to string variable with that funny name:
      "__doc__".
    </p>

    <p>
      So where does this get changed into something more familiar?  I’m
      glad you asked! Because, this comes in the next phase
      ... [*]
    </p>

    <p>Phase 4 takes the parse tree we just created and transforms
      that into an abstract syntax tree. We look for tree patterns
      like this special kind of assignment statement. When the process
      finds something of interest that matches, that part of the tree
      is replaced. In a sense, we are "abstracting" the specifics we
      found into something that is more the way the Python programmer
      wrote the program.</p>

    <p>
      Strictly speaking, we don't have to do this here, and sometimes this
      kind of thing can be a difference between the reconstructed and
      the original source text. However, If you find Python source
      text that looks screwy or isn’t even valid Python, what may be
      going on is that this kind of transformation isn’t being
      detected.
    </p>

    <p>
      Notice that we have also simplified the tree as well. We no
      longer have grammar symbols "expr" or "store".
    </p>

    <p>
      We could have tried to detect docstrings earlier, but that would
      have been messier.
    </p>

    <p>
      And finally we get to that actual source text.  The
      source text is created by traversing this AST, calling routines
      based on the name of the grammar symbol, here the grammar symbol
      name is "docstring" in yellow. [*]
    </p>

    <p>
      To simplify the conversion of the AST to a source string, there
      is a custom domain-specific language or DSL used. This DSL is
      described in the project wiki.
    </p>
    <hr/>

    <!--- 120-intermission.html #/12 -->
    <h1>Intermission</h1>
    <p>You have now successfully completed the
      first part of the talk which gives an overview.</p>

    <p>
      What's coming up may be rougher.</p>
    <p>I next go over one disassembly in more detail, and a
      new idea for the decompiler world which uses some
      lesser-known compiler technology. [*]
    </p>
    <hr/>

    <!--- 130-bytecode-disassembly.html #/13 -->
    <h1>Bytecode Disassembly &mdash; Sequence of instructions</h1>

      <p>I’m going to focus a little more on disassembly for one
	important reason: There aren’t good decompilers for the
	most-recent Python-bytecode versions. And that
	is likely to continue forever. So the
	reality is: you may have to understand bytecode malware from a
	disassembly listing.
      </p>

      <p>
	There is a disassember in the standard Python library
	called <em>dis</em>. Most novice decompilers use this&mdash;
	it's the first thing that comes to mind. But that has some
	serious limitations.  The biggest limitation is that it can
	only disassemble code for a single Python version,
	the version that runs the decompiler. If you are running the
	latest version of Python, such 3.12, but the bytecode you want
	to analyze is from an earlier version, like bytecode from 2.7,
	then you are out of luck.  Malware written in Python tends to
	use older versions of Python. This was the situation
	when Microsoft folks contacted me. [*]
      </p>

      <p>To get around this limitation, <em>uncompyle6</em>
	and <em>decompile3</em> use <em>xdis</em> &mdash; the
	"cross-Python disassembler".
      </p>

    <p>
      <em>xdis</em> has a disassembler on steroids that I’m going to
      show now briefly. [*]
    </p>

    <p>
      Command-line options in yellow of the disassembler that I use in this
      invocation are:
      <ul>
	<li>basic interpretation of sequences of
	  instructions</li>
	<li>showing the actual bytecode</li>
	<li>the ability to intermingle the source-code text with
	  assembly.</li>
      </ul>
    </p>

    <p>
      In reverse engineering, most of the time you will not have the
      source text. There is, of course, no harm in asking for source
      text when it doesn’t exist.  Now let's look at the
      output. [*]
    </p>

    <!--- 130-bytecode-disassembly.html #/13/1 -->
    <h1>Bytecode Disassembly</h1>
    <p> Again,
      At the top, again, is some metadata in orange. I went into some
      of this earlier, so I have elided this.
    </p>

    <p>
      A piece of metadata that we saw implicitly before was
      this value from this constants table.
      The first instruction at offset 0 is "LOAD_CONST". The string
      operand of this instruction came from index zero of this
      constants table.  Now let's compare the assembly instructions.
    </p>

    <p>
      If you are familiar with Python's <em>dis</em> output, this is
      similar. But there is more information.  First, we see the
      Python source text, in green, for line 1. It is that Python
      docstring.
    </p>

    <p>
      Below that, in white, we have, between vertical bars, the actual
      bytecode values. This is also at the top of the slide in
      hex.</p>

    <p>The first byte of the first instruction is hex 64. This is the
      opcode in Python 3.8, for "LOAD_CONST". The operand value is 0,
      but 0 is an index into that orange constants table
      above.
    </p>

    <p>
      In the second instruction, we see something interesting
      happening off to the far right...  This is after the
      parenthesized operand value, <em>__doc__</em> in green. There is
      additional text &mdash; an assignment statement that starts in
      white: <pre>__doc__ = 'BlackHat Asia Example'</pre>
    </p>

    <p>
      Here, the disassembler is describing the combined
      effect of the first
      two lines. As I mentioned before, this is how Python
      implements docstrings.
    </p>

    <p>
     Here is a more complicated example. Companion materials explain
     this in more detail.
    </p>
    <hr/>

    <p>Don't get confused between <em>disassembly</em>
      and <em>decomplation</em>. Even though there is some
      higher-level interpretation, what we have here is a
      <em>disassembly</em>. Internals of the interpreter are
      exposed. For example, the "Top of Stack" or "toss equal" refer
      how the interpreter works. And the "POP_TOP"
      instruction which is greyed out, just below the last
      CALL_FUNCTION, has no corresponding meaning in the source
      code.
    </p>

    <p>
      Disassembly is easy and straight-forward. Decompilation is
      hard. However, decompilation starts out with a disassembly of
      the bytecode.
    </p>
    <hr/>

    <!--- 140-chained-compare-cfg-code.html #/14 -->
    <h1>Chained Compare Bytecode</h1>

    <p>
      However awesome the disassemble of in the last slide is, it has
      limitations.
      Here is an example of something that disassembly can't
      reconstruct.
    </p>
    <p>

      The disassembler is great with code that doesn't
      have jumps in it.
      But once we have jumps, like we have in the highlighted instructions, a
      disassembler has to stop combining instructions.
    </p>

    <p>
      With each new Python release, decompilation of control
      flow has become increasingly more difficult.
      Nowadays, about 1/3 of the issues raised in the decompiler bug
      trackers are related to control flow.
    </p>

    <p>
      Grammars and the grammar-based approach we use, however, can
      naturally parse nesting and sequencing control structures
      quite well. So having a methodical way to characterize control
      flow that fits into the decompiler parsing can give more
      precision and accuracy than is available in
      general-purpose decompilers [*]
    </p>
    <hr/>

    <!--- 140-chained-compare-cfg-code.html #/14/1 -->
    <h1>Chained Compare Parse Tree (new code)</h1>
    <p>
      I won’t say much about how
      <em>uncompyle6</em> and <em>decompile3</em> handle difficulties
      that arise in disambiguating control-flow patterns.
    </p>

    <p>
      Instead, I will be using the approach in an experimental
      decompiler I’m working on. Below, is the AST from this
      experimental decompiler. [*]
    </p>


    <p>
      The tokens in white: "BB_START", "BB_END", "SIBLING_BLOCK" and
      "BLOCK_END_JOIN" are pseudo tokens. They get inserted
      by a special control-flow process that I will show, briefly,
      next.
    </p>

    <p>By the end of those slides, I hope you will understand a little
      about "Basic Blocks " that you see this in orange. "BB" in the
      token name is an abbreviation for "Basic Block". And I hope you
      get some idea of what "SIBLING" and "JOIN" mean in the token names,
      and what "Dominator" means in the orange operand. [*]
    <hr/>

    <!--- 150-control-flow.html #/15 -->
    <h1>Classifying Scopes and Important Control-Flow Points</h1>

    <p>Let’s now consider this simple Python program to compute the
      number of 1 bits in a positive integer.  If you are interested
      in algorithms, you might find this code interesting. But the
      only thing that concerns us when thinking about the control
      flow, is that it is a program with the following
      characteristics:
    </p>

    <p>Nested inside the main body of code is a <em>while</em> loop
      ...
    </p>

    <p>
      Call this nesting level one. But nested inside
      the <em>while</em> loop ...  there is an <em>if/then/else</em>
      statement.
    </p>

    <p>Call this nesting level two.</p>

    <p> There is one more nesting level. ...
    </p>

    <p>... the <em>then</em> and <em>else</em> parts of
    the <em>if</em>. In contrast to the above nesting,
    we have more than one item at the same nesting
    level. Even though the two blocks are at the
    same nesting level, the they are distinct
    and separate. In some programming languages like Java and
    JavaScript, variables declared in each block would be in
    different scopes.</p>

    <p>
      What we want to do to make parsing easier is detect this
      nesting level, or more precisely, the scope. And mark
      scope boundaries inside the instruction sequence.
    </p>

    <p>That brings us to these orange comments. These capture what I
      just said. But I’m doing this in the source text rather than in
      the bytecode-instruction sequence. Note that we need to mark
      both the beginning and the end of sequenced and nested regions.
    </p>

    <p>
      In order to get to any code with the same block scope as one of
      the orange instructions which has the word "dominator", you have
      to pass through the "dominator" comment. Similarly, in order get
      to any instruction that follows the scope you have to
      pass through the matching comment that says "join".
    </p>

    <p>So now, let’s look at how the Python interpreter
    sees things...
    </p>
    <hr/>


    <!--- 150-control-flow.html #/15/1 -->
    <h1>Control Flow Produced
      from <a ref="https://github.com/rocky/python-control-flow"><code style="font-size:smaller">control_flow</code></a></h1>

    <p>
      This is the way the Python interpreter sees the control
      flow for the bytecode instructions for that Python text.
    </p>

    <p>
      This graph comes from the Python <em>control_flow</em> project.
      I created this project to perform more accurate and faster
      decompilation of Python control-flow structures. However, this
      project can be used for other kinds of Python bytecode
      control-flow analysis.
    </p>

    <p>First, instructions are broken into "basic blocks". A basic
      block is straight-line code. There are no jumps into the middle
      of the block, only the to beginning of the block. If there is a
      jump anywhere in the sequence of instructions, it
      has to be at the last instruction.
    </p>

    <p>
      I don’t have time to describe the text in the boxes or what the
      colors and line styles mean. For that, look at companion
      materials.
    </p>

    <hr/>

    <!--- 150-control-flow.html #/15/2 -->
    <h1>Dominator Regions and Dominators</h1>
    <p>
      Here, we have the control-flow graph from before, but now
      altered to show dominators regions and dominators. This
      graph comes out of a later phase of the control-flow
      project.
    </p>
    <p>There is a lot going on here, so let me break this down.
      Let me first define "dominator region" and "dominator".
    </p>

    <p>
      The easiest term to explain is "dominator region". This is just
      another term for "scope" described in the last slide.  But it is
      a property that is computed on a directed graph, or
      more precisely a control-flow graph like this. This property is
      used quite a bit in the compiler data-flow analysis. See books
      on advanced compiler optimization for algorithms on how to
      compute this.
    </p>

    <p>What does the "dominator" part of "dominator region"
      mean? A dominator block is a block that you have to
      pass through from the start of the program to get to anything
      underneath it. It acts as a gatekeeper. In other words, the
      instruction or block dominates other instructions or
      blocks underneath it. For example, to get to any instruction in
      the "count bits" program, you have to go through Basic Block 1
      which contains the first instruction of the program. That has
      the 3D box shape to indicate the block’s special role as a
      dominator block. Technically, a block dominates itself. When
      that is all it dominates, we don’t show the block in
      3D. Only if it dominates another block do we decorate
      the block in 3D.</p>

    <p>To run an instruction in either Block 4 or Block 5 in brown,
      you have to pass through the all instructions in the
      other brown block: Block 3. Block 3 then dominates Blocks 3,
      4, and 5.</p>

    <p>But Block 3 is dominated by both lighter-colored
      Blocks 2 and 1.  The hierarchy of dominator blocks is capturing
      the nesting level in the "count bits" program.
    </p>

    <p>Notice that Blocks 3, 4, and 5 all have the same shade of
      brown. This means they are all part of the same "dominator
      region", and have scopes under the scope of Dominator Block 3.
    </p>

    <p>But notice Block 7 is a lighter tan color, the same color as
      Block 2, Therefore, you don’t have to go through
      darker-color Block 3 to get to Block 7.  Although
      is possible by looping around to get there, that
      doesn’t have to happen. In contrast, to get to Block 7
      you must go through the parent dominator block for that
      region, Block 2.
    </p>

    <p>The darker the color a block is, the more nested it is inside
      the other block regions it is. So block 3 is nested or dominated
      inside block regions 2 and 1.
    </p>

    <p>I wave my hands wildly here. There are a lot more
      details to cover, especially in the practical setting of a
      decompiler. But I hope you can see how a program can capture
      block nesting versus alternation and the instruction
      level. Those wacky pseudo-tokens inserted into the token stream
      in the "chained comparison" example, are there to convey this
      nesting and sequencing information in a token stream format so
      that parser can understand and match blocks of code. I have an
      unpublished technical paper that explains this in more more
      detail.
    </p>
    <hr/>

    <!--- 160-general-stuff.html #/16 -->
    <h1>General Remarks</h1>

    <p>
      Now that I have given a whirlwind tour of bytecode decompilation
      for these Python decompilers, let me back up and put this in
      place.
    </p>

    <p>
      There are other decompilers for Python. They all start out with
      a disassembly, even the one I looked at that uses machine
      learning. Many build a tree&mdash;more or less&mdash;based
      on instructions from a disasssembly, and they all produce source
      text from that internal tree-ish structure.</p>

      <p>However, they are a bit more ad hoc. None use the
      grammar-based approach here. The phases are fewer and a little
      less distinct. [*]
    </p>

     <p>
      General-purpose decompilers like you find in Ghidra are largely
      different. They live in a more complicated world. To be able to
      do things across a wider spectrum of machine languages and a
      wider spectrum of programming-language environments, they
      largely give up on the hope of noticing specific patterns of
      instructions, like we showed in the chained-comparison
      example. The ability to match specific patterns is what makes
      these decompilers produce extremely intuitive and accurate
      results and written in the programming language that the source
      text was also written in. [*]
    </p>

    <p>
      Control flow in general-purpose decompilers is its own canned
      phase. This phase doesn’t take into account the specific target
      programming language that produced the code and the specific set
      of control-flow structures that the source language has.
    </p>

    <p>
      Our control flow is intimately tied to the control flow for a
      particular Python version. When Python added a new construct like
      "async" co-routines in Python 3.5, it added a new kind of
      control-flow pattern match. Python has an extremely rich
      set of control-flow structures. I know of no canned
      control-flow-detection mechanism that would be able to
      cover all control-flow mechanisms that Python includes
      like the "else" clauses on "while", "for", and "try"
      blocks.
    </p>

    <p>
      Our approach uses tokenization to facilitated parsing. This is
      similar to the lift phase that general-purpose decompilers
      often do after initial disassembly . In general-purpose
      decompilers, the lifting language is sometimes to LLVM or an
      LLVM-like language. In Python, our intermediate language is very
      much tied to Python bytecode. In general, that is true for all
      high-level bytecode decompilers. The intermediate code looks
      like the high-level bytecode. Also, this intermediate language
      drifts over time along with the language and bytecode drift.
    </p>
    <hr/>

    <!--- 170-wrap-up.html #/17 -->
    <h1>Wrapping Up</h1>
    <p>
      I have shown how two of the best and most-popular Python
      decompilers work.  Of course, these have bugs in them. But by
      understanding how they work, hopefully you may be able to:
      <ol>
	<li>Understand, pinpoint and report bugs better, or
	even fix problems.</li>
	<li>Begin to understand Python bytecode in general,
	and how the source text and its bytecode are
	related.</li>

	<li>Extend this code for newer or unsupported Python bytecode.
	</li>

	<li>Use these techniques to write a decompiler for
	  other programming languages, such as Ethereum's Solidity,
	  Lua, Ruby, or various LISPs. For the bytecode you may be
	  interested in, there may not be a decompiler available
	  yet. Hopefully with this information you are in a position
	  to write one.</li>
      </ol>
    </p>
    </hr>

    <!--- 180-thanks.html #/18 -->

    <h1>Thanks</h1>

    <p>It was John Aycott who first had the idea for using a
      grammar-based approach for a toy Python decompiler.  Hartmut
      Goebels wrote the first version that covered a full version of
      Python. In fact, it handed several Python releases from Python
      1.5 to about Python 2.3. [*]
    </p>

    <p>I would like to thank the organizers and program-selection
      committee for inviting me to give this talk, and then making
      sure that things went smoothly after it was accepted.
    </p>

    <p>I have many projects that are in need of attention. So I have
      to figure out which ones work to on. Therefore, when a talk like
      this is accepted, it is encouraging. It shows support for
      projects that I have volunteered a lot my time on, and have
      worked hard on for many years.</p>

    <p>
      In preparation for this talk, I go over the software that I will
      be talking about add new features and remove bugs. [*]</p>

    <p>I would like to thank Phil Young for coaching me on this talk. I
      spend a much more time writing code, than I do speaking about the
      code I've written. Phil has been invaluable. He has had many
      great ideas for organizing and presenting this in a more lively,
      and audience-friendly way.</p>

    <p>Stuart Frankel edited the talk, and turned some of this into
    English.</p>

    <p>Also, I thank Lidia Giuliano for organizing and running the
      speaker-coaching program. [*]
    </p>

    <p>Finally, that you all for listening to me talk about this
    subject. I hope you all will use this information to produce great
    things in the future.
    </p>
    <hr/>

    <!--- 190-references.html #/19 -->
    <h1>Additional Information</h1>
    <p>Here is additional information for where you can get even more
      detail about things I've said today. Links are clickable in the
      slides and in the PDF.
    </p>

    <ul style="font-size: smaller">
      <li>Python Decompilers: <a href="https://pypi.org/project/uncompyle6/">https://pypi.org/project/uncompyle6</a>, and <a href="https://pypi.org/project/decompyle3/">https://pypi.org/project/decompyle3/</a></li>
      <li>Cross-Version Python Disassembler: <a href="https://pypi.org/project/xdis/">https://pypi.org/project/xdis</a></li>
      <li>Python Control Flow: <a href="https://github.com/rocky/python-control-flow">https://github.com/rocky/python-control-flow</a></li>
      <li>Older Decompiler Paper: <a href="https://rocky.github.io/Deparsing-Paper.pdf">https://rocky.github.io/Deparsing-Paper.pdf</a></li>
      <li>PyCon Columbia 2018 talk slides: <a href="https://rocky.github.io/pycon2018-light.co">https://rocky.github.io/pycon2018-light.co</a></li>
      <li>PyCon Columbia 2018 talk video: <a href="https://www.youtube.com/watch?v=bRQr1OroXUM&feature=youtu.be">https://www.youtube.com/watch?v=bRQr1OroXUM&feature=youtu.be</a></li>
      <li>Text of this talk: <a href="https://rocky.github.io/blackhat-asia-2024-additional/all-notes-print"</a>https://rocky.github.io/blackhat-asia-2024-additional/all-notes-print</li>
      <li>Additional Slides: <a href="https://rocky.github.io/blackhat-asia-2024-additional"</a>https://rocky.github.io/blackhat-asia-2024-additional</li>
    </ul>


    <p>Thanks, Everybody!  [FIN]

</body>
</html>
